{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps\n",
    "**Spike removal / filtering methods**\n",
    "<br>\n",
    "    -- Reduction of spike events by special design of the instrument (Zhao, 2003)\n",
    "<br>\n",
    "    -- Automatic Spike Removal Algorithm for Raman Spectra: wavelet transform (spike removal raman filter from matlab)\n",
    "<br>\n",
    "    -- Missing point polynomial filter (I have the code)\n",
    "<br>\n",
    "    -- Robust smoothing filter\n",
    "<br>\n",
    "    -- Moving window filter \n",
    "<br>\n",
    "**Remove background Autofluorescence noise**\n",
    "<br>\n",
    "--IModPoly (Chad A Lieber and Anita Mahadevan-Jansen. Automated method for subtraction offluorescence from biological raman spectra.Applied spectroscopy, 57(11):1363â€“1367,2003) (https://github.com/michaelstchen/modPolyFit)(Faster technique)\n",
    " <br>\n",
    "--Zhiming Zhang (An intelligent background-correction algorithm for highly fluorescent samples in raman spectroscopy: https://onlinelibrary.wiley.com/doi/abs/10.1002/jrs.2500)(https://github.com/zmzhang/baselineWavelet)\n",
    "<br>\n",
    "--Vancouver Raman Algorithm (Jianhua Zhao: http://journals.sagepub.com/doi/abs/10.1366/000370207782597003) \n",
    "<br>\n",
    "--EMD (Empirical  Mode Decomposition) (https://github.com/laszukdawid/PyEMD)\n",
    "<br>\n",
    "**Smoothing (Denoising)**\n",
    "<br>\n",
    "-- Savisky-Golay filtering (Scipi package):  https://github.com/scipy/scipy/blob/master/scipy/signal/_savitzky_golay.py\n",
    "<br>\n",
    "-- Moving Average/median\n",
    "<br>\n",
    "--CARS (Coherent Anti-Stokes Raman spectroscopy) \n",
    "<br>\n",
    "**Normalize**\n",
    "<br>\n",
    "--Min/Max method (I have the code).\n",
    "<br>\n",
    "--Vector based \n",
    "<br>\n",
    "**Spectral and intensity re-calibration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal**\n",
    "<br>\n",
    "Individual patients with 5 sample points in blood is 381\n",
    "<br>\n",
    "Individual patients with 3 sample points in blood is 228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disease 1:**\n",
    "\n",
    "Individual patients with 5 sample points in blood is 144.\n",
    "<br>\n",
    "Individual patients with 3 sample points in blood is 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class dealing with the Raman data\n",
    "'''\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from convertwdf import *\n",
    "from wdfReader import * \n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, rmsprop\n",
    "#%matplotlib inline \n",
    "#https://github.com/MacDumi/Deconvolution\n",
    "#python3 Deconvolution_test.py /home/titli/Documents/Deconvolution-master/0151.txt \n",
    "#https://www.pnas.org/content/114/31/8247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    _min = np.min(data)\n",
    "    _max = np.max(data)\n",
    "    return (data - _min) / (_max - _min)\n",
    "def getspikes(fileID):\n",
    "    \n",
    "    x_data= fileID.get_xdata()\n",
    "    spectra= fileID.get_spectra()\n",
    "    return (x_data, spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_array_1 = [] #patients in disease1\n",
    "patient_array_0 = [] #patients in disease0\n",
    "spectra_array0_980 = [] #spectrum in disease0\n",
    "spectra_array0_1700 = [] #spectrum in disease1\n",
    "spectra_array1_980 = [] #spectrum in disease1\n",
    "spectra_array1_1700 = [] #spectrum in disease1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual patients 381\n"
     ]
    }
   ],
   "source": [
    "rootdir = '/home/titli/Documents/normal'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[5] == '1_0-5-1'):\n",
    "            if (str(x[7]) not in patient_array_0):\n",
    "                patient_array_0.append(x[7])  \n",
    "patient_array_0 = list(set(patient_array_0))\n",
    "print('Individual patients', len(patient_array_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual patients 144\n"
     ]
    }
   ],
   "source": [
    "rootdir = '/home/titli/Documents/disease1'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[5] == '1_0-5-1'):\n",
    "            if (str(x[7]) not in patient_array_1):\n",
    "                patient_array_1.append(x[7]) \n",
    "patient_array_1 = list(set(patient_array_1))\n",
    "print('Individual patients', len(patient_array_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual date-patient 330\n"
     ]
    }
   ],
   "source": [
    "date_list_1 = []\n",
    "rootdir = '/home/titli/Documents/disease1'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        date_list_1.append(subdir)\n",
    "date_list_1 = list(set(date_list_1))\n",
    "print('Individual date-patient', len(date_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual date-patient 1213\n"
     ]
    }
   ],
   "source": [
    "date_list_0 = []\n",
    "rootdir = '/home/titli/Documents/normal'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        date_list_0.append(subdir)\n",
    "date_list_0 = list(set(date_list_0))\n",
    "print('Individual date-patient', len(date_list_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spectra_1 = []\n",
    "for dirnames in date_list_1:\n",
    "    names = dirnames.split(\"/\")\n",
    "    if (str(names[7]) in patient_array_1 and str(names[8]== 980)):\n",
    "        path, dirs, files = next(os.walk(dirnames)) # get all the file names\n",
    "        newdirnames = '/'.join(names[:-1])+'/1700'\n",
    "        # looping through file names\n",
    "        for f in files:\n",
    "            filenames980 = dirnames + \"/\"+f\n",
    "            filenames1700 = newdirnames + \"/\"+f\n",
    "            wdfIle980 = wdfReader(filenames980)\n",
    "            X, spectra980 = getspikes(wdfIle980)\n",
    "            try:\n",
    "                wdfIle1700 = wdfReader(filenames1700)\n",
    "                X, spectra1700 = getspikes(wdfIle1700)\n",
    "            except OSError:\n",
    "                wdfIle1700 = np.zeros(1015)\n",
    "            total_spectra_1.append(np.append(normalize(spectra980), normalize(spectra1700)))     \n",
    "        try:\n",
    "            patient_array_1.remove(names[7])\n",
    "        except ValueError:\n",
    "            continue\n",
    "total_df_1= pd.DataFrame(total_spectra_1)\n",
    "labels_1 =  pd.DataFrame({'labels': np.ones(len(total_df_1))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spectra_0 = []\n",
    "for dirnames in date_list_0:\n",
    "    names = dirnames.split(\"/\")\n",
    "    if (str(names[7]) in patient_array_0 and str(names[8]== 980)): \n",
    "        path, dirs, files = next(os.walk(dirnames)) # get all the file names\n",
    "        newdirnames = '/'.join(names[:-1])+'/1700'\n",
    "        # looping through file names\n",
    "        for f in files:\n",
    "            filenames980 = dirnames + \"/\"+f\n",
    "            filenames1700 = newdirnames + \"/\"+f\n",
    "            wdfIle980 = wdfReader(filenames980)\n",
    "            X, spectra980 = getspikes(wdfIle980)\n",
    "            try:\n",
    "                wdfIle1700 = wdfReader(filenames1700)\n",
    "                X, spectra1700 = getspikes(wdfIle1700)\n",
    "            except OSError:\n",
    "                wdfIle1700 = np.ones(1015)*10000\n",
    "            total_spectra_1.append(np.append(normalize(spectra980), normalize(spectra1700)))     \n",
    "            total_spectra_0.append(np.append(normalize(spectra980), normalize(spectra1700)))   \n",
    "        try:\n",
    "            patient_array_0.remove(names[7])\n",
    "        except ValueError:\n",
    "            continue\n",
    "total_df_0= pd.DataFrame(total_spectra_0)\n",
    "labels_0 =  pd.DataFrame({'labels': np.zeros(len(total_df_0))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-validation split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_train = pd.concat([total_df_1[:int(len(total_df_1)*0.8)],total_df_0[:int(len(total_df_0)*0.8)]], axis = 0)\n",
    "total_df_train = total_df_train.apply(lambda x: [y if y <= 1e-5 else 1e-4 for y in x])\n",
    "labels_df_train = pd.concat([labels_1[:int(len(total_df_1)*0.8)],labels_0[:int(len(total_df_0)*0.8)]], axis = 0)\n",
    "indices=list(range(0,len(total_df_train)))\n",
    "random.shuffle(indices)\n",
    "X_train = total_df_train.values[indices].reshape(total_df_train.shape[0],total_df_train.shape[1],1)\n",
    "y_train = labels_df_train.values[indices]\n",
    "y_train_labels = to_categorical(y_train, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "560+1524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "140+381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_val = pd.concat([total_df_1[int(len(total_df_1)*0.8):],total_df_0[int(len(total_df_0)*0.8):]], axis = 0)\n",
    "total_df_val = total_df_val.apply(lambda x: [y if y <= 1e-5 else 1e-4 for y in x])\n",
    "labels_df_val = pd.concat([labels_1[int(len(total_df_1)*0.8):],labels_0[int(len(total_df_0)*0.8):]], axis = 0)\n",
    "indices=list(range(0,len(total_df_val)))\n",
    "random.shuffle(indices)\n",
    "X_val = total_df_val.values[indices].reshape(total_df_val.shape[0],total_df_val.shape[1],1)\n",
    "y_val = labels_df_val.values[indices]\n",
    "y_val_labels = to_categorical(y_val, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kraub_method():\n",
    "    inp =  Input(shape=(2030, 1))\n",
    "    x = Conv1D(32, kernel_size = 7, strides= 1,padding='valid', activation='relu')(inp)\n",
    "    x = Conv1D(16, kernel_size = 5, strides= 1, padding='valid', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.01)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.01)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inp, preds)\n",
    "    model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer= 'rmsprop',\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_model_step2.hdf5\".format('boat_detector')\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) \n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2084 samples, validate on 521 samples\n",
      "Epoch 1/65\n",
      "2084/2084 [==============================] - 51s 25ms/step - loss: 0.6025 - acc: 0.7265 - val_loss: 0.5835 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58347, saving model to boat_detector_model_step2.hdf5\n",
      "Epoch 2/65\n",
      "2084/2084 [==============================] - 52s 25ms/step - loss: 0.5844 - acc: 0.7313 - val_loss: 0.5820 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58347 to 0.58198, saving model to boat_detector_model_step2.hdf5\n",
      "Epoch 3/65\n",
      "2084/2084 [==============================] - 51s 24ms/step - loss: 0.5864 - acc: 0.7313 - val_loss: 0.5820 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58198\n",
      "Epoch 4/65\n",
      "2084/2084 [==============================] - 50s 24ms/step - loss: 0.5853 - acc: 0.7313 - val_loss: 0.5824 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58198\n",
      "Epoch 5/65\n",
      "2084/2084 [==============================] - 50s 24ms/step - loss: 0.5860 - acc: 0.7313 - val_loss: 0.5822 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58198\n",
      "Epoch 6/65\n",
      "2084/2084 [==============================] - 51s 24ms/step - loss: 0.5844 - acc: 0.7313 - val_loss: 0.5852 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58198\n",
      "Epoch 7/65\n",
      "2084/2084 [==============================] - 52s 25ms/step - loss: 0.5843 - acc: 0.7313 - val_loss: 0.5850 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58198\n",
      "Epoch 8/65\n",
      "2084/2084 [==============================] - 52s 25ms/step - loss: 0.5849 - acc: 0.7313 - val_loss: 0.5820 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58198\n",
      "Epoch 9/65\n",
      "2084/2084 [==============================] - 52s 25ms/step - loss: 0.5835 - acc: 0.7313 - val_loss: 0.5916 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.58198\n",
      "Epoch 10/65\n",
      "2084/2084 [==============================] - 52s 25ms/step - loss: 0.5840 - acc: 0.7313 - val_loss: 0.5824 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.58198\n",
      "Epoch 11/65\n",
      "2084/2084 [==============================] - 53s 25ms/step - loss: 0.5843 - acc: 0.7313 - val_loss: 0.5857 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.58198\n",
      "Epoch 12/65\n",
      "2084/2084 [==============================] - 52s 25ms/step - loss: 0.5832 - acc: 0.7313 - val_loss: 0.5824 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.58198\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n"
     ]
    }
   ],
   "source": [
    "model = kraub_method()\n",
    "history = model.fit(X_train, y_train_labels, batch_size= 5, epochs=65, validation_data=(X_val, y_val_labels),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_step3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_step3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_array_1 = [] #patients in disease1\n",
    "patient_array_0 = [] #patients in disease0\n",
    "patient_array_2 = [] #patients in disease1\n",
    "patient_array_3 = [] #patients in disease0\n",
    "spectra_array0_980 = [] #spectrum in disease0\n",
    "spectra_array0_1700 = [] #spectrum in disease1\n",
    "spectra_array1_980 = [] #spectrum in disease1\n",
    "spectra_array1_1700 = [] #spectrum in disease1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/home/titli/Documents/test/disease1'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[6] == '1_0-5-1'):\n",
    "            if (str(x[8]) not in patient_array_1):\n",
    "                patient_array_1.append(x[8]) \n",
    "patient_array_1= set(patient_array_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list_1 = []\n",
    "rootdir = '/home/titli/Documents/test/disease1'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        date_list_1.append(subdir)\n",
    "date_list_1 = list(set(date_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/home/titli/Documents/test/normal'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[6] == '1_0-5-1'):\n",
    "            if (str(x[8]) not in patient_array_0):\n",
    "                patient_array_0.append(x[8])    \n",
    "patient_array_0= set(patient_array_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list_0 = []\n",
    "rootdir = '/home/titli/Documents/test/normal'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        date_list_0.append(subdir)\n",
    "date_list_0 = list(set(date_list_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spectra_1 = []\n",
    "for dirnames in date_list_1:\n",
    "    names = dirnames.split(\"/\")\n",
    "    if ( str(names[9]== 980) and str(names[8]) in patient_array_1): #str(names[8]) in patient_array_1 and\n",
    "        path, dirs, files = next(os.walk(dirnames)) # get all the file namesone\n",
    "        newdirnames = '/'.join(names[:-1])+'/1700'\n",
    "        # looping through file names\n",
    "        for f in files:\n",
    "            \n",
    "            filenames980 = dirnames + \"/\"+f\n",
    "            filenames1700 = newdirnames + \"/\"+f\n",
    "            wdfIle980 = wdfReader(filenames980)\n",
    "            X, spectra980 = getspikes(wdfIle980)\n",
    "            try:\n",
    "                wdfIle1700 = wdfReader(filenames1700)\n",
    "                X, spectra1700 = getspikes(wdfIle1700)\n",
    "            except OSError:\n",
    "                wdfIle1700 = np.ones(1015)*10000\n",
    "            total_spectra_1.append(np.append(normalize(spectra980), normalize(spectra1700))) \n",
    "        try:\n",
    "            patient_array_1.remove(names[8])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "total_df_1_test= pd.DataFrame(total_spectra_1)\n",
    "labels_test_1 =  pd.DataFrame({'labels': np.ones(len(total_df_1_test))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spectra_0 = []\n",
    "for dirnames in date_list_0:\n",
    "    names = dirnames.split(\"/\")\n",
    "    if (str(names[8]) in patient_array_0 and str(names[9]== 980)):\n",
    "        path, dirs, files = next(os.walk(dirnames)) # get all the file names\n",
    "        newdirnames = '/'.join(names[:-1])+'/1700'\n",
    "        # looping through file names\n",
    "        for f in files:\n",
    "            filenames980 = dirnames + \"/\"+f\n",
    "            filenames1700 = newdirnames + \"/\"+f\n",
    "            wdfIle980 = wdfReader(filenames980)\n",
    "            X, spectra980 = getspikes(wdfIle980)\n",
    "            wdfIle1700 = wdfReader(filenames1700)\n",
    "            X, spectra1700 = getspikes(wdfIle1700)\n",
    "            total_spectra_0.append(np.append(normalize(spectra980), normalize(spectra1700)))   \n",
    "        try:\n",
    "            patient_array_0.remove(names[8])\n",
    "        except ValueError:\n",
    "            continue\n",
    "total_df_0_test= pd.DataFrame(total_spectra_0)\n",
    "labels_test_0 =  pd.DataFrame({'labels': np.zeros(len(total_df_0_test))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "total_df_test = pd.concat([total_df_1_test,total_df_0_test], axis = 0)\n",
    "total_df_test = total_df_test.apply(lambda x: [y if y <= 1e-5 else 1e-4 for y in x])\n",
    "X_test = total_df_test.values\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "labels_df_test = pd.concat([labels_test_0,labels_test_1], axis = 0)\n",
    "y_test = labels_df_test.values\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "model1_test_y = model.predict(X_test, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_test_y[model1_test_y > 0.5] = 1\n",
    "model1_test_y[model1_test_y <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(pred_test_y, actuals):\n",
    "\n",
    "    predictions =[]\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for i in range (len(pred_test_y)):\n",
    "        if ((pred_test_y[i,0]==1) & (actuals[i,0]==1)):\n",
    "            true_pos = true_pos+1\n",
    "        elif((pred_test_y[i,0]==0) & (actuals[i,0]==0)):\n",
    "            true_neg = true_neg+1\n",
    "        elif((pred_test_y[i,0]==1) & (actuals[i,0]==0)):\n",
    "            false_pos = false_pos +1\n",
    "        elif((pred_test_y[i,0]==0) & (actuals[i,0]==1)):\n",
    "            false_neg = false_neg+1\n",
    "    #prec=true_pos/(true_pos+false_pos)\n",
    "    recall = true_pos/(true_pos+false_neg)\n",
    "    accur=(true_pos+true_neg)/(true_pos+false_pos+ true_neg+ false_neg)\n",
    "    #F1=2*(prec*recall/(prec+recall))\n",
    "    #FPR = false_pos/(false_pos+true_neg)\n",
    "    return (true_pos, false_pos, true_neg, false_neg, accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 100, 0, 0, 0.8181818181818182)\n"
     ]
    }
   ],
   "source": [
    "print((F1_score(model1_test_y, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
