{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps\n",
    "**Spike removal / filtering methods**\n",
    "<br>\n",
    "    -- Reduction of spike events by special design of the instrument (Zhao, 2003)\n",
    "<br>\n",
    "    -- Automatic Spike Removal Algorithm for Raman Spectra: wavelet transform (spike removal raman filter from matlab)\n",
    "<br>\n",
    "    -- Missing point polynomial filter (I have the code)\n",
    "<br>\n",
    "    -- Robust smoothing filter\n",
    "<br>\n",
    "    -- Moving window filter \n",
    "<br>\n",
    "**Remove background Autofluorescence noise**\n",
    "<br>\n",
    "--IModPoly (Chad A Lieber and Anita Mahadevan-Jansen. Automated method for subtraction offluorescence from biological raman spectra.Applied spectroscopy, 57(11):1363â€“1367,2003) (https://github.com/michaelstchen/modPolyFit)(Faster technique)\n",
    " <br>\n",
    "--Zhiming Zhang (An intelligent background-correction algorithm for highly fluorescent samples in raman spectroscopy: https://onlinelibrary.wiley.com/doi/abs/10.1002/jrs.2500)(https://github.com/zmzhang/baselineWavelet)\n",
    "<br>\n",
    "--Vancouver Raman Algorithm (Jianhua Zhao: http://journals.sagepub.com/doi/abs/10.1366/000370207782597003) \n",
    "<br>\n",
    "--EMD (Empirical  Mode Decomposition) (https://github.com/laszukdawid/PyEMD)\n",
    "<br>\n",
    "**Smoothing (Denoising)**\n",
    "<br>\n",
    "-- Savisky-Golay filtering (Scipi package):  https://github.com/scipy/scipy/blob/master/scipy/signal/_savitzky_golay.py\n",
    "<br>\n",
    "-- Moving Average/median\n",
    "<br>\n",
    "--CARS (Coherent Anti-Stokes Raman spectroscopy) \n",
    "<br>\n",
    "**Normalize**\n",
    "<br>\n",
    "--Min/Max method (I have the code).\n",
    "<br>\n",
    "--Vector based \n",
    "<br>\n",
    "**Spectral and intensity re-calibration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal**\n",
    "<br>\n",
    "Individual patients with 5 sample points in blood is 471\n",
    "<br>\n",
    "Individual patients with 3 sample points in blood is 228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disease 1:**\n",
    "\n",
    "Individual patients with 5 sample points in blood is 153.\n",
    "<br>\n",
    "Individual patients with 3 sample points in blood is 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Class dealing with the Raman data\n",
    "'''\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from convertwdf import *\n",
    "from wdfReader import * \n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, rmsprop\n",
    "#%matplotlib inline \n",
    "#https://github.com/MacDumi/Deconvolution\n",
    "#python3 Deconvolution_test.py /home/titli/Documents/Deconvolution-master/0151.txt \n",
    "#https://www.pnas.org/content/114/31/8247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    _min = np.min(data)\n",
    "    _max = np.max(data)\n",
    "    return (data - _min) / (_max - _min)\n",
    "def getspikes(fileID):\n",
    "    \n",
    "    x_data= fileID.get_xdata()\n",
    "    spectra= fileID.get_spectra()\n",
    "    return (x_data, spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_array_1 = [] #patients in disease1\n",
    "patient_array_0 = [] #patients in disease0\n",
    "spectra_array0 = [] #spectrum in disease0\n",
    "spectra_array1 = [] #spectrum in disease1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disease 1\n",
    "rootdir = '/home/titli/Documents/disease1'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[5] == '1_0-5-1' and x[8] == '980'):\n",
    "            if (str(x[7]) not in patient_array_1):\n",
    "                patient_array_1.append(x[7])\n",
    "                wdfIle = wdfReader(os.path.join(subdir, file))\n",
    "                X, spectra = getspikes(wdfIle) # plotting the spectrum\n",
    "                if len(X)<1015:\n",
    "                    continue\n",
    "                spectra = normalize(spectra)\n",
    "                spectra_array1.append(spectra)\n",
    "spectra_array_1= pd.DataFrame(spectra_array1)\n",
    "labels_1 = pd.DataFrame({'labels': np.ones((len(spectra_array1),), dtype=int)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/home/titli/Documents/normal'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[5] == '1_0-5-1' and x[8] == '980'):\n",
    "            if (str(x[7]) not in patient_array_0):\n",
    "                patient_array_0.append(x[7])\n",
    "                wdfIle = wdfReader(os.path.join(subdir, file))\n",
    "                X, spectra = getspikes(wdfIle) # plotting the spectrum\n",
    "                if len(X)<1015:\n",
    "                    continue\n",
    "                spectra = normalize(spectra)\n",
    "                spectra_array0.append(spectra)\n",
    "spectra_array_0= pd.DataFrame(spectra_array0)\n",
    "labels_0 = pd.DataFrame({'labels': np.zeros((len(spectra_array0),), dtype=int)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat([spectra_array_0,spectra_array_1], axis = 0)\n",
    "labels_df = pd.concat([labels_0,labels_1], axis = 0)\n",
    "indices=list(range(0,len(total_df)))\n",
    "random.shuffle(indices)\n",
    "X = total_df.values[indices]\n",
    "y = labels_df.values[indices]\n",
    "#total_df.fillna(0.0)\n",
    "#len(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test- Train Dataset: Making a balanced dataset 50 disease1 and 50 normal\n",
    "split_val= int(len(X)*0.8)\n",
    "X_train=X[:split_val]\n",
    "X_test=X[split_val:,]\n",
    "y_train =y[:split_val]\n",
    "y_test =y[split_val:]\n",
    "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train_labels = to_categorical(y_train, num_classes=2)\n",
    "y_test_labels = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kraub_method():\n",
    "    inp =  Input(shape=(1015, 1))\n",
    "    x = Conv1D(32, kernel_size = 7, strides= 1,padding='valid', activation='relu')(inp)\n",
    "    x = Conv1D(16, kernel_size = 5, strides= 1, padding='valid', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.01)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.01)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inp, preds)\n",
    "    model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer= 'rmsprop',\n",
    "              metrics=['acc'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_model_step1.hdf5\".format('boat_detector')\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) \n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 381 samples, validate on 96 samples\n",
      "Epoch 1/65\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.6266 - acc: 0.8110 - val_loss: 0.6805 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68052, saving model to boat_detector_model_step1.hdf5\n",
      "Epoch 2/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.4719 - acc: 0.8425 - val_loss: 0.5673 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68052 to 0.56730, saving model to boat_detector_model_step1.hdf5\n",
      "Epoch 3/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.4134 - acc: 0.8530 - val_loss: 0.4274 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56730 to 0.42742, saving model to boat_detector_model_step1.hdf5\n",
      "Epoch 4/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3733 - acc: 0.8530 - val_loss: 0.6840 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.42742\n",
      "Epoch 5/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3833 - acc: 0.8609 - val_loss: 0.4703 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.42742\n",
      "Epoch 6/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3693 - acc: 0.8583 - val_loss: 1.0085 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.42742\n",
      "Epoch 7/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3593 - acc: 0.8714 - val_loss: 0.5031 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.42742\n",
      "Epoch 8/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3580 - acc: 0.8635 - val_loss: 0.6650 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.42742\n",
      "Epoch 9/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3335 - acc: 0.8714 - val_loss: 0.5237 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42742\n",
      "Epoch 10/65\n",
      "381/381 [==============================] - 3s 8ms/step - loss: 0.3343 - acc: 0.8688 - val_loss: 1.4684 - val_acc: 0.5104\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42742\n",
      "Epoch 11/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3156 - acc: 0.8766 - val_loss: 0.8139 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42742\n",
      "Epoch 12/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3130 - acc: 0.8714 - val_loss: 0.5818 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42742\n",
      "Epoch 13/65\n",
      "381/381 [==============================] - 3s 7ms/step - loss: 0.3021 - acc: 0.8845 - val_loss: 0.5415 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.42742\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n"
     ]
    }
   ],
   "source": [
    "model = kraub_method()\n",
    "history = model.fit(X_train, y_train_labels, batch_size= 10, epochs=65, validation_data=(X_test, y_test_labels),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_step1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "#model.save_weights(\"model_step1.h5\")\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disease 1\n",
    "patient_array_1 = [] #patients in disease1\n",
    "patient_array_0 = [] #patients in disease0\n",
    "spectra_array0 = [] #spectrum in disease0\n",
    "spectra_array1 = [] #spectrum in disease1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disease 1\n",
    "rootdir = '/home/titli/Documents/test/disease1'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[6] == '1_0-5-1' and x[9] == '980'):\n",
    "            if (str(x[8]) not in patient_array_1):\n",
    "                patient_array_1.append(x[8])\n",
    "                wdfIle = wdfReader(os.path.join(subdir, file))\n",
    "                X, spectra = getspikes(wdfIle) # plotting the spectrum\n",
    "                if len(X)<1015:\n",
    "                    continue\n",
    "                spectra = normalize(spectra)\n",
    "                spectra_array1.append(spectra)\n",
    "spectra_array_1= pd.DataFrame(spectra_array1)\n",
    "labels_test_1 = pd.DataFrame({'labels': np.ones((len(spectra_array1),), dtype=int)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectra_array_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/home/titli/Documents/test/normal'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print (os.path.join(subdir, file))\n",
    "        txt = os.path.join(subdir, file)\n",
    "        x = txt.split(\"/\")\n",
    "        if( x[6] == '1_0-5-1' and x[9] == '980'):\n",
    "            if (str(x[8]) not in patient_array_0):\n",
    "                patient_array_0.append(x[8])\n",
    "                wdfIle = wdfReader(os.path.join(subdir, file))\n",
    "                X, spectra = getspikes(wdfIle) # plotting the spectrum\n",
    "                if len(X)<1015:\n",
    "                    continue\n",
    "                spectra = normalize(spectra)\n",
    "                spectra_array0.append(spectra)\n",
    "spectra_array_0= pd.DataFrame(spectra_array0)\n",
    "labels_test_0 = pd.DataFrame({'labels': np.zeros((len(spectra_array0),), dtype=int)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "total_df_test = pd.concat([spectra_array_0,spectra_array_1], axis = 0)\n",
    "X_test = total_df_test.values\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "labels_df_test = pd.concat([labels_test_0,labels_test_1], axis = 0)\n",
    "y_test = labels_df_test.values\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "model1_test_y = model.predict(X_test, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_test_y[model1_test_y > 0.5] = 1\n",
    "model1_test_y[model1_test_y <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(pred_test_y, actuals):\n",
    "\n",
    "    predictions =[]\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for i in range (len(pred_test_y)):\n",
    "        if ((pred_test_y[i,0]==1) & (actuals[i,0]==1)):\n",
    "            true_pos = true_pos+1\n",
    "        elif((pred_test_y[i,0]==0) & (actuals[i,0]==0)):\n",
    "            true_neg = true_neg+1\n",
    "        elif((pred_test_y[i,0]==1) & (actuals[i,0]==0)):\n",
    "            false_pos = false_pos +1\n",
    "        elif((pred_test_y[i,0]==0) & (actuals[i,0]==1)):\n",
    "            false_neg = false_neg+1\n",
    "    prec=true_pos/(true_pos+false_pos)\n",
    "    recall = true_pos/(true_pos+false_neg)\n",
    "    accur=(true_pos+true_neg)/(true_pos+false_pos+ true_neg+ false_neg)\n",
    "    #F1=2*(prec*recall/(prec+recall))\n",
    "    #FPR = false_pos/(false_pos+true_neg)\n",
    "    return (true_pos, false_pos, true_neg, false_neg, accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 0, 0, 2, 0.9777777777777777)\n"
     ]
    }
   ],
   "source": [
    "print((F1_score(model1_test_y, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
